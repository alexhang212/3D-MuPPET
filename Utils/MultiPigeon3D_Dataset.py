import os
from collections import defaultdict
import pickle

import numpy as np
import cv2

import torch
from torch.utils.data import Dataset


import sys
sys.path.append("Repositories/learnable-triangulation-pytorch/")
sys.path.append("./")

from mvn.utils.multiview import Camera
from mvn.utils.img import get_square_bbox, resize_image, crop_image, normalize_image, scale_bbox
from mvn.utils import volumetric

from Utils import JSONReader

class MultiPigeon3D_Dataset(Dataset):
    """
        Dataset class for multi pigeon multi view 
    """
    def __init__(self,
                 DatasetPath, 
                 JSONPath,
                 Type, 
                 pred_results_path=None,
                 image_shape=(256, 256),
                 train=False,
                 test=False,
                 retain_every_n_frames_in_test=1,
                 with_damaged_actions=False,
                 cuboid_side=350,
                 scale_bbox=1.5,
                 norm_image=True,
                 kind="mpii",
                 undistort_images=False,
                 ignore_cameras=[],
                 crop=True
                 ):
        """
            h36m_root:
                Path to 'processed/' directory in Human3.6M
            labels_path:
                Path to 'human36m-multiview-labels.npy' generated by 'generate-labels-npy-multiview.py'
                from https://github.sec.samsung.net/RRU8-VIOLET/human36m-preprocessing
            retain_every_n_frames_in_test:
                By default, there are 159 181 frames in training set and 26 634 in test (val) set.
                With this parameter, test set frames will be evenly skipped frames so that the
                test set size is `26634 // retain_every_n_frames_test`.
                Use a value of 13 to get 2049 frames in test set.
            with_damaged_actions:
                If `True`, will include 'S9/[Greeting-2,SittingDown-2,Waiting-1]' in test set.
            kind:
                Keypoint format, 'mpii' or 'human36m'
            ignore_cameras:
                A list with indices of cameras to exclude (0 to 3 inclusive)
        """
        self.Dataset = JSONReader.JSONReader(JSONPath,DatasetPath, Type = "3D")
        
        self.CamNames = ["Cam1","Cam2","Cam3","Cam4"]
        self.scale_bbox = scale_bbox
        self.DatasetPath= DatasetPath
        self.crop = crop
        self.image_shape = None if image_shape is None else tuple(image_shape)
        self.norm_image = norm_image
        self.cuboid_side = cuboid_side
        
        

        
        # assert train or test, '`Human36MMultiViewDataset` must be constructed with at least ' \
        #                       'one of `test=True` / `train=True`'
        # assert kind in ("mpii", "human36m")

        # self.h36m_root = h36m_root
        # self.labels_path = labels_path
        # self.image_shape = None if image_shape is None else tuple(image_shape)
        # self.scale_bbox = scale_bbox
        # self.norm_image = norm_image
        # self.cuboid_side = cuboid_side
        # self.kind = kind
        # self.undistort_images = undistort_images
        # self.ignore_cameras = ignore_cameras
        # self.crop = crop

        # self.labels = np.load(labels_path, allow_pickle=True).item()

        # n_cameras = len(self.labels['camera_names'])
        # assert all(camera_idx in range(n_cameras) for camera_idx in self.ignore_cameras)

        # train_subjects = ['S1', 'S5', 'S6', 'S7', 'S8']
        # test_subjects = ['S9', 'S11']

        # train_subjects = list(self.labels['subject_names'].index(x) for x in train_subjects)
        # test_subjects  = list(self.labels['subject_names'].index(x) for x in test_subjects)

        # indices = []
        # if train:
        #     mask = np.isin(self.labels['table']['subject_idx'], train_subjects, assume_unique=True)
        #     indices.append(np.nonzero(mask)[0])
        # if test:
        #     mask = np.isin(self.labels['table']['subject_idx'], test_subjects, assume_unique=True)

        #     if not with_damaged_actions:
        #         mask_S9 = self.labels['table']['subject_idx'] == self.labels['subject_names'].index('S9')

        #         damaged_actions = 'Greeting-2', 'SittingDown-2', 'Waiting-1'
        #         damaged_actions = [self.labels['action_names'].index(x) for x in damaged_actions]
        #         mask_damaged_actions = np.isin(self.labels['table']['action_idx'], damaged_actions)

        #         mask &= ~(mask_S9 & mask_damaged_actions)

        #     indices.append(np.nonzero(mask)[0][::retain_every_n_frames_in_test])

        # self.labels['table'] = self.labels['table'][np.concatenate(indices)]

        # self.num_keypoints = 16 if kind == "mpii" else 17
        # assert self.labels['table']['keypoints'].shape[1] == 17, "Use a newer 'labels' file"

        # self.keypoints_3d_pred = None
        # if pred_results_path is not None:
        #     pred_results = np.load(pred_results_path, allow_pickle=True)
        #     keypoints_3d_pred = pred_results['keypoints_3d'][np.argsort(pred_results['indexes'])]
        #     self.keypoints_3d_pred = keypoints_3d_pred[::retain_every_n_frames_in_test]
        #     assert len(self.keypoints_3d_pred) == len(self), \
        #         f"[train={train}, test={test}] {labels_path} has {len(self)} samples, but '{pred_results_path}' " + \
        #         f"has {len(self.keypoints_3d_pred)}. Did you follow all preprocessing instructions carefully?"

    def __len__(self):
        return len(self.Dataset.Annotations)

    def __getitem__(self, idx):
        #Default dict template:
        sample = defaultdict(list) # return value
        
        FrameData = self.Dataset.Annotations[idx]
        
        # shot = self.labels['table'][idx]
        # subject = self.labels['subject_names'][shot['subject_idx']]
        # action = self.labels['action_names'][shot['action_idx']]
        # frame_idx = shot['frame_idx']

        for camera_idx, camera_name in enumerate(self.CamNames):
            # load bounding box
            bbox = self.Dataset.ExtractBBox(idx)[camera_idx]
            # bbox = shot['bbox_by_camera_tlbr'][camera_idx][[1,0,3,2]] # TLBR to LTRB
            bbox_height = bbox[2] - bbox[0]
            if bbox_height == 0:
                # convention: if the bbox is empty, then this view is missing
                continue

            # scale the bounding box
            bbox = scale_bbox(bbox, self.scale_bbox)

            # load image
            img_relativePath = self.Dataset.GetImagePath(idx)[camera_idx]
            image_path = os.path.join(self.DatasetPath,img_relativePath)
            assert os.path.isfile(image_path), '%s doesn\'t exist' % image_path
            image = cv2.imread(image_path)

            # load camera
            rvec,tvec = self.Dataset.GetExtrinsics(idx)
            camMat,distCoef = self.Dataset.GetIntrinsics(idx)
            retval_camera = Camera(R = cv2.Rodrigues(rvec[camera_idx])[0], 
                                   t= tvec[camera_idx], 
                                   K = camMat[camera_idx], 
                                   dist = distCoef[camera_idx], 
                                   name= camera_name)

            if self.crop:
                # crop image
                image = crop_image(image, bbox)
                retval_camera.update_after_crop(bbox)

            if self.image_shape is not None:
                # resize
                image_shape_before_resize = image.shape[:2]
                image = resize_image(image, self.image_shape)
                retval_camera.update_after_resize(image_shape_before_resize, self.image_shape)

                sample['image_shapes_before_resize'].append(image_shape_before_resize)

            if self.norm_image:
                image = normalize_image(image)
                
                
            ##add arbitiary dimensino to image?
            # image = np.expand_dims(image,axis=0)

            sample['images'].append(image)
            sample['detections'].append(bbox + (1.0,)) # TODO add real confidences
            sample['cameras'].append(retval_camera)
            sample['proj_matrices'].append(retval_camera.projection)

        # 3D keypoints
        # add dummy confidences
        Points3D = list(self.Dataset.Extract3D(idx).values())
        
        sample['keypoints_3d']=np.pad(Points3D,((0,0), (0,1)), 'constant', constant_values=1.0)


        # build cuboid
        base_point = sample['keypoints_3d'][2, :3]
        sides = np.array([self.cuboid_side, self.cuboid_side, self.cuboid_side])
        position = base_point - sides / 2
        sample['cuboids'] = volumetric.Cuboid3D(position, sides)

        # save sample's index
        sample['indexes'] = idx

        # if self.keypoints_3d_pred is not None:
        #     sample['pred_keypoints_3d'] = self.keypoints_3d_pred[idx]

        sample.default_factory = None
        return sample

    def evaluate_using_per_pose_error(self, per_pose_error, split_by_subject):
        
        def evaluate_by_actions(self, per_pose_error, mask=None):
            if mask is None:
                mask = np.ones_like(per_pose_error, dtype=bool)

            action_scores = {
                'Average': {'total_loss': per_pose_error[mask].sum(), 
                            'frame_count': np.count_nonzero(mask),
                            'Average':per_pose_error[mask].sum()/np.count_nonzero(mask)
                            }
            }

            ###NO ACTION TO SCORE HERE

            # for action_idx in range(len(self.labels['action_names'])):
            #     action_mask = (self.labels['table']['action_idx'] == action_idx) & mask
            #     action_per_pose_error = per_pose_error[action_mask]
            #     action_scores[self.labels['action_names'][action_idx]] = {
            #         'total_loss': action_per_pose_error.sum(), 'frame_count': len(action_per_pose_error)
            #     }

            # action_names_without_trials = \
            #     [name[:-2] for name in self.labels['action_names'] if name.endswith('-1')]

            # for action_name_without_trial in action_names_without_trials:
            #     combined_score = {'total_loss': 0.0, 'frame_count': 0}

            #     for trial in 1, 2:
            #         action_name = '%s-%d' % (action_name_without_trial, trial)
            #         combined_score['total_loss' ] += action_scores[action_name]['total_loss']
            #         combined_score['frame_count'] += action_scores[action_name]['frame_count']
            #         del action_scores[action_name]

            #     action_scores[action_name_without_trial] = combined_score

            # for k, v in action_scores.items():
            #     action_scores[k] = float('nan') if v['frame_count'] == 0 else (v['total_loss'] / v['frame_count'])

            return action_scores
        # import ipdb;ipdb.set_trace
        subject_scores = {
            'Average': evaluate_by_actions(self, per_pose_error)
        }

        # for subject_idx in range(len(self.labels['subject_names'])):
        #     subject_mask = self.labels['table']['subject_idx'] == subject_idx
        #     subject_scores[self.labels['subject_names'][subject_idx]] = \
        #         evaluate_by_actions(self, per_pose_error, subject_mask)

        return subject_scores

    def evaluate(self, keypoints_3d_predicted, Indexes, split_by_subject=False, transfer_cmu_to_human36m=False, transfer_human36m_to_human36m=False):
        """get differences between ground truth and predicted"""
        # import ipdb;ipdb.set_trace()
        keypoints_gt = self.Dataset.GetGTArray(Indexes)
        
        # keypoints_gt = self.labels['table']['keypoints'][:, :self.num_keypoints]
        if keypoints_3d_predicted.shape != keypoints_gt.shape:
            raise ValueError(
                '`keypoints_3d_predicted` shape should be %s, got %s' % \
                (keypoints_gt.shape, keypoints_3d_predicted.shape))

        # if transfer_cmu_to_human36m or transfer_human36m_to_human36m:
        #     human36m_joints = [10, 11, 15, 14, 1, 4]
        #     if transfer_human36m_to_human36m:
        #         cmu_joints = [10, 11, 15, 14, 1, 4]
        #     else:
        #         cmu_joints = [10, 8, 9, 7, 14, 13]

            # keypoints_gt = keypoints_gt[:, human36m_joints]
            # keypoints_3d_predicted = keypoints_3d_predicted[:, cmu_joints]

        # mean error per 16/17 joints in mm, for each pose
        per_pose_error = np.sqrt(((keypoints_gt - keypoints_3d_predicted) ** 2).sum(2)).mean(1)

        # relative mean error per 16/17 joints in mm, for each pose
        # if not (transfer_cmu_to_human36m or transfer_human36m_to_human36m):
        #     root_index = 6 if self.kind == "mpii" else 6
        # else:
        root_index = 2 #this is index of all points relative to this, for human 3.6M is pelvis.
        ##Now set to 2, which is top keel--> bot keel (in the middle)

        keypoints_gt_relative = keypoints_gt - keypoints_gt[:, root_index:root_index + 1, :]
        keypoints_3d_predicted_relative = keypoints_3d_predicted - keypoints_3d_predicted[:, root_index:root_index + 1, :]

        per_pose_error_relative = np.sqrt(((keypoints_gt_relative - keypoints_3d_predicted_relative) ** 2).sum(2)).mean(1)

        result = {
            'per_pose_error': self.evaluate_using_per_pose_error(per_pose_error, split_by_subject),
            'per_pose_error_relative': self.evaluate_using_per_pose_error(per_pose_error_relative, split_by_subject)
        }
        # import ipdb;ipdb.set_trace()

        return result['per_pose_error_relative']['Average']["Average"]["Average"], result




class POP3D_Dataset(Dataset):
    """
        Dataset class for 3D pop, reads in 1 frame worth of data (for multi ind)
        Each index is for each individual in frame
        
    """
    def __init__(self, pred_results_path=None,
                 image_shape=(256, 256),
                 train=False,
                 test=False,
                 retain_every_n_frames_in_test=1,
                 with_damaged_actions=False,
                 cuboid_side=350,
                 scale_bbox=1.5,
                 norm_image=True,
                 kind="mpii",
                 undistort_images=False,
                 ignore_cameras=[],
                 crop=True,
                 Dataset = None,
                 Frame = 0,
                 BBoxIndex = None
                 ):
        """
            h36m_root:
                Path to 'processed/' directory in Human3.6M
            labels_path:
                Path to 'human36m-multiview-labels.npy' generated by 'generate-labels-npy-multiview.py'
                from https://github.sec.samsung.net/RRU8-VIOLET/human36m-preprocessing
            retain_every_n_frames_in_test:
                By default, there are 159 181 frames in training set and 26 634 in test (val) set.
                With this parameter, test set frames will be evenly skipped frames so that the
                test set size is `26634 // retain_every_n_frames_test`.
                Use a value of 13 to get 2049 frames in test set.
            with_damaged_actions:
                If `True`, will include 'S9/[Greeting-2,SittingDown-2,Waiting-1]' in test set.
            kind:
                Keypoint format, 'mpii' or 'human36m'
            ignore_cameras:
                A list with indices of cameras to exclude (0 to 3 inclusive)
        """
        
        Dataset.load3DPopDataset() #load in data
        self.SeqeunceObj = Dataset
        self.CamObjects = Dataset.camObjects
        self.SubjectNames = Dataset.Subjects
        self.Frame = Frame
        self.CamNames = ["Cam1","Cam2","Cam3","Cam4"]
        self.scale_bbox = scale_bbox
        # self.DatasetPath= DatasetPath
        self.crop = crop
        self.image_shape = None if image_shape is None else tuple(image_shape)
        self.norm_image = norm_image
        self.BBoxIndex = BBoxIndex ##the indecies of subjects that have bbox data
        self.cuboid_side = cuboid_side

    def __len__(self):
        return len(self.BBoxIndex)

    def __getitem__(self, idx):
        RealIndex = self.BBoxIndex[idx] #actual bird index in bird list
        #Default dict template:
        sample = defaultdict(list) # return value
        
        # FrameData = self.Dataset.Annotations[idx]
        
        # shot = self.labels['table'][idx]
        # subject = self.labels['subject_names'][shot['subject_idx']]
        # action = self.labels['action_names'][shot['action_idx']]
        # frame_idx = shot['frame_idx']

        for camera_idx, camera_name in enumerate(self.CamNames):
            camObj = self.CamObjects[camera_idx]
            # load bounding box
            bbox = camObj.GetBBoxData(camObj.BBox,self.Frame, self.SubjectNames[RealIndex])
            bbox = [bbox[0][0],bbox[0][1],bbox[1][0],bbox[1][1]]
            # bbox = shot['bbox_by_camera_tlbr'][camera_idx][[1,0,3,2]] # TLBR to LTRB
            bbox_height = bbox[3] - bbox[1]
            bbox_width = bbox[2] - bbox[0]
            if bbox_height == 0 or bbox_width == 0:
                # convention: if the bbox is empty, then this view is missing
                continue

            # scale the bounding box
            bbox = scale_bbox(bbox, self.scale_bbox)

            # load image
            cap = cv2.VideoCapture(camObj.VideoPath)
            cap.set(cv2.CAP_PROP_POS_FRAMES,self.Frame)
            ret,image = cap.read()
            
            # load camera
            rvec = camObj.rvec
            tvec = camObj.tvec
            camMat = camObj.camMat
            distCoef = camObj.distCoef
            retval_camera = Camera(R = cv2.Rodrigues(rvec)[0], 
                                   t= tvec, 
                                   K = camMat, 
                                   dist = distCoef, 
                                   name= camera_name)

            if self.crop:
                # crop image
                image = crop_image(image, bbox)
                retval_camera.update_after_crop(bbox)

            if self.image_shape is not None:
                # resize
                image_shape_before_resize = image.shape[:2]
                image = resize_image(image, self.image_shape)
                retval_camera.update_after_resize(image_shape_before_resize, self.image_shape)

                sample['image_shapes_before_resize'].append(image_shape_before_resize)

            if self.norm_image:
                image = normalize_image(image)
                
                
            ##add arbitiary dimensino to image?
            # image = np.expand_dims(image,axis=0)

            sample['images'].append(image)
            sample['detections'].append(bbox + (1.0,)) # TODO add real confidences
            sample['cameras'].append(retval_camera)
            sample['proj_matrices'].append(retval_camera.projection)

        # import ipdb;ipdb.set_trace()
        # 3D keypoints
        # add dummy confidences
        Points3D = camObj.Read3DKeypointData(camObj.Keypoint3D,self.Frame,self.SubjectNames[RealIndex])
        # import ipdb;ipdb.set_trace()
        Points3D = [v for k,v in Points3D.items() if k[-1].isdigit() == False ]


        # Points3D  = np.zeros((9,3))
        #Set all points to zero except root point (Top Keel)
        for i in range(len(Points3D)):
            if i != 2:
                Points3D[i] = [0,0,0]

        
        sample['keypoints_3d']=np.pad(Points3D,((0,0), (0,1)), 'constant', constant_values=1.0)

        
        # build cuboid
        base_point = sample['keypoints_3d'][2, :3]
        sides = np.array([self.cuboid_side, self.cuboid_side, self.cuboid_side])
        position = base_point - sides / 2
        sample['cuboids'] = volumetric.Cuboid3D(position, sides)

        
        # save sample's index
        sample['indexes'] = idx

        # if self.keypoints_3d_pred is not None:
        #     sample['pred_keypoints_3d'] = self.keypoints_3d_pred[idx]

        sample.default_factory = None
        return sample

    def evaluate_using_per_pose_error(self, per_pose_error, split_by_subject):
        
        def evaluate_by_actions(self, per_pose_error, mask=None):
            if mask is None:
                mask = np.ones_like(per_pose_error, dtype=bool)

            action_scores = {
                'Average': {'total_loss': per_pose_error[mask].sum(), 
                            'frame_count': np.count_nonzero(mask),
                            'Average':per_pose_error[mask].sum()/np.count_nonzero(mask)
                            }
            }

            ###NO ACTION TO SCORE HERE

            # for action_idx in range(len(self.labels['action_names'])):
            #     action_mask = (self.labels['table']['action_idx'] == action_idx) & mask
            #     action_per_pose_error = per_pose_error[action_mask]
            #     action_scores[self.labels['action_names'][action_idx]] = {
            #         'total_loss': action_per_pose_error.sum(), 'frame_count': len(action_per_pose_error)
            #     }

            # action_names_without_trials = \
            #     [name[:-2] for name in self.labels['action_names'] if name.endswith('-1')]

            # for action_name_without_trial in action_names_without_trials:
            #     combined_score = {'total_loss': 0.0, 'frame_count': 0}

            #     for trial in 1, 2:
            #         action_name = '%s-%d' % (action_name_without_trial, trial)
            #         combined_score['total_loss' ] += action_scores[action_name]['total_loss']
            #         combined_score['frame_count'] += action_scores[action_name]['frame_count']
            #         del action_scores[action_name]

            #     action_scores[action_name_without_trial] = combined_score

            # for k, v in action_scores.items():
            #     action_scores[k] = float('nan') if v['frame_count'] == 0 else (v['total_loss'] / v['frame_count'])

            return action_scores
        # import ipdb;ipdb.set_trace
        subject_scores = {
            'Average': evaluate_by_actions(self, per_pose_error)
        }

        # for subject_idx in range(len(self.labels['subject_names'])):
        #     subject_mask = self.labels['table']['subject_idx'] == subject_idx
        #     subject_scores[self.labels['subject_names'][subject_idx]] = \
        #         evaluate_by_actions(self, per_pose_error, subject_mask)

        return subject_scores

    def evaluate(self, keypoints_3d_predicted, Indexes, split_by_subject=False, transfer_cmu_to_human36m=False, transfer_human36m_to_human36m=False):
        """get differences between ground truth and predicted"""
        # import ipdb;ipdb.set_trace()
        keypoints_gt = self.Dataset.GetGTArray(Indexes)
        
        # keypoints_gt = self.labels['table']['keypoints'][:, :self.num_keypoints]
        if keypoints_3d_predicted.shape != keypoints_gt.shape:
            raise ValueError(
                '`keypoints_3d_predicted` shape should be %s, got %s' % \
                (keypoints_gt.shape, keypoints_3d_predicted.shape))

        # if transfer_cmu_to_human36m or transfer_human36m_to_human36m:
        #     human36m_joints = [10, 11, 15, 14, 1, 4]
        #     if transfer_human36m_to_human36m:
        #         cmu_joints = [10, 11, 15, 14, 1, 4]
        #     else:
        #         cmu_joints = [10, 8, 9, 7, 14, 13]

            # keypoints_gt = keypoints_gt[:, human36m_joints]
            # keypoints_3d_predicted = keypoints_3d_predicted[:, cmu_joints]

        # mean error per 16/17 joints in mm, for each pose
        per_pose_error = np.sqrt(((keypoints_gt - keypoints_3d_predicted) ** 2).sum(2)).mean(1)

        # relative mean error per 16/17 joints in mm, for each pose
        # if not (transfer_cmu_to_human36m or transfer_human36m_to_human36m):
        #     root_index = 6 if self.kind == "mpii" else 6
        # else:
        root_index = 2 #this is index of all points relative to this, for human 3.6M is pelvis.
        ##Now set to 2, which is top keel--> bot keel (in the middle)

        keypoints_gt_relative = keypoints_gt - keypoints_gt[:, root_index:root_index + 1, :]
        keypoints_3d_predicted_relative = keypoints_3d_predicted - keypoints_3d_predicted[:, root_index:root_index + 1, :]

        per_pose_error_relative = np.sqrt(((keypoints_gt_relative - keypoints_3d_predicted_relative) ** 2).sum(2)).mean(1)

        result = {
            'per_pose_error': self.evaluate_using_per_pose_error(per_pose_error, split_by_subject),
            'per_pose_error_relative': self.evaluate_using_per_pose_error(per_pose_error_relative, split_by_subject)
        }
        # import ipdb;ipdb.set_trace()

        return result['per_pose_error_relative']['Average']["Average"]["Average"], result


class POP3D_Dataset_AlgIn(Dataset):
    """
        Dataset class for 3D pop, reads in 1 frame worth of data (for multi ind)
        Each index is for each individual in frame
        Reads in input from algorithmic triangulation net as root point
        
    """
    def __init__(self, pred_results_path=None,
                 image_shape=(256, 256),
                 Points3D = None,
                 train=False,
                 test=False,
                 retain_every_n_frames_in_test=1,
                 with_damaged_actions=False,
                 cuboid_side=350,
                 scale_bbox=1.5,
                 norm_image=True,
                 kind="mpii",
                 undistort_images=False,
                 ignore_cameras=[],
                 crop=True,
                 Dataset = None,
                 Frame = 0,
                 BBoxIndex = None
                 ):
        """
            h36m_root:
                Path to 'processed/' directory in Human3.6M
            labels_path:
                Path to 'human36m-multiview-labels.npy' generated by 'generate-labels-npy-multiview.py'
                from https://github.sec.samsung.net/RRU8-VIOLET/human36m-preprocessing
            retain_every_n_frames_in_test:
                By default, there are 159 181 frames in training set and 26 634 in test (val) set.
                With this parameter, test set frames will be evenly skipped frames so that the
                test set size is `26634 // retain_every_n_frames_test`.
                Use a value of 13 to get 2049 frames in test set.
            with_damaged_actions:
                If `True`, will include 'S9/[Greeting-2,SittingDown-2,Waiting-1]' in test set.
            kind:
                Keypoint format, 'mpii' or 'human36m'
            ignore_cameras:
                A list with indices of cameras to exclude (0 to 3 inclusive)
        """
        
        Dataset.load3DPopDataset() #load in data
        self.SeqeunceObj = Dataset
        self.CamObjects = Dataset.camObjects
        self.SubjectNames = Dataset.Subjects
        self.Frame = Frame
        self.CamNames = ["Cam1","Cam2","Cam3","Cam4"]
        self.scale_bbox = scale_bbox
        # self.DatasetPath= DatasetPath
        self.crop = crop
        self.image_shape = None if image_shape is None else tuple(image_shape)
        self.norm_image = norm_image
        self.BBoxIndex = BBoxIndex ##the indecies of subjects that have bbox data
        self.cuboid_side = cuboid_side
        self.Points3D = Points3D

    def __len__(self):
        return len(self.BBoxIndex)

    def __getitem__(self, idx):
        RealIndex = self.BBoxIndex[idx] #actual bird index in bird list
        #Default dict template:
        sample = defaultdict(list) # return value
        
        # FrameData = self.Dataset.Annotations[idx]
        
        # shot = self.labels['table'][idx]
        # subject = self.labels['subject_names'][shot['subject_idx']]
        # action = self.labels['action_names'][shot['action_idx']]
        # frame_idx = shot['frame_idx']

        for camera_idx, camera_name in enumerate(self.CamNames):
            camObj = self.CamObjects[camera_idx]
            # load bounding box
            bbox = camObj.GetBBoxData(camObj.BBox,self.Frame, self.SubjectNames[RealIndex])
            bbox = [bbox[0][0],bbox[0][1],bbox[1][0],bbox[1][1]]
            # bbox = shot['bbox_by_camera_tlbr'][camera_idx][[1,0,3,2]] # TLBR to LTRB
            bbox_height = bbox[3] - bbox[1]
            bbox_width = bbox[2] - bbox[0]
            if bbox_height == 0 or bbox_width == 0:
                # convention: if the bbox is empty, then this view is missing
                continue

            # scale the bounding box
            bbox = scale_bbox(bbox, self.scale_bbox)

            # load image
            cap = cv2.VideoCapture(camObj.VideoPath)
            cap.set(cv2.CAP_PROP_POS_FRAMES,self.Frame)
            ret,image = cap.read()
            
            # load camera
            rvec = camObj.rvec
            tvec = camObj.tvec
            camMat = camObj.camMat
            distCoef = camObj.distCoef
            retval_camera = Camera(R = cv2.Rodrigues(rvec)[0], 
                                   t= tvec, 
                                   K = camMat, 
                                   dist = distCoef, 
                                   name= camera_name)

            if self.crop:
                # crop image
                image = crop_image(image, bbox)
                retval_camera.update_after_crop(bbox)

            if self.image_shape is not None:
                # resize
                image_shape_before_resize = image.shape[:2]
                image = resize_image(image, self.image_shape)
                retval_camera.update_after_resize(image_shape_before_resize, self.image_shape)

                sample['image_shapes_before_resize'].append(image_shape_before_resize)

            if self.norm_image:
                image = normalize_image(image)
                
                
            ##add arbitiary dimensino to image?
            # image = np.expand_dims(image,axis=0)

            sample['images'].append(image)
            sample['detections'].append(bbox + (1.0,)) # TODO add real confidences
            sample['cameras'].append(retval_camera)
            sample['proj_matrices'].append(retval_camera.projection)

        # import ipdb;ipdb.set_trace()
        # 3D keypoints
        # add dummy confidences
        # Points3D = camObj.Read3DKeypointData(camObj.Keypoint3D,self.Frame,self.SubjectNames[RealIndex])
        # import ipdb;ipdb.set_trace()
        # Points3D = [v for k,v in Points3D.items() if k[-1].isdigit() == False ]

        # import ipdb;ipdb.set_trace()
        Pt3D  = self.Points3D[RealIndex].cpu().numpy()
        #Set all points to zero except root point (Top Keel)
        for i in range(len(Pt3D)):
            if i != 2:
                Pt3D[i] = [0,0,0]

        
        sample['keypoints_3d']=np.pad(Pt3D,((0,0), (0,1)), 'constant', constant_values=1.0)

        
        # build cuboid
        base_point = sample['keypoints_3d'][2, :3]
        sides = np.array([self.cuboid_side, self.cuboid_side, self.cuboid_side])
        position = base_point - sides / 2
        sample['cuboids'] = volumetric.Cuboid3D(position, sides)

        
        # save sample's index
        sample['indexes'] = idx

        # if self.keypoints_3d_pred is not None:
        #     sample['pred_keypoints_3d'] = self.keypoints_3d_pred[idx]

        sample.default_factory = None
        return sample

    def evaluate_using_per_pose_error(self, per_pose_error, split_by_subject):
        
        def evaluate_by_actions(self, per_pose_error, mask=None):
            if mask is None:
                mask = np.ones_like(per_pose_error, dtype=bool)

            action_scores = {
                'Average': {'total_loss': per_pose_error[mask].sum(), 
                            'frame_count': np.count_nonzero(mask),
                            'Average':per_pose_error[mask].sum()/np.count_nonzero(mask)
                            }
            }

            ###NO ACTION TO SCORE HERE

            # for action_idx in range(len(self.labels['action_names'])):
            #     action_mask = (self.labels['table']['action_idx'] == action_idx) & mask
            #     action_per_pose_error = per_pose_error[action_mask]
            #     action_scores[self.labels['action_names'][action_idx]] = {
            #         'total_loss': action_per_pose_error.sum(), 'frame_count': len(action_per_pose_error)
            #     }

            # action_names_without_trials = \
            #     [name[:-2] for name in self.labels['action_names'] if name.endswith('-1')]

            # for action_name_without_trial in action_names_without_trials:
            #     combined_score = {'total_loss': 0.0, 'frame_count': 0}

            #     for trial in 1, 2:
            #         action_name = '%s-%d' % (action_name_without_trial, trial)
            #         combined_score['total_loss' ] += action_scores[action_name]['total_loss']
            #         combined_score['frame_count'] += action_scores[action_name]['frame_count']
            #         del action_scores[action_name]

            #     action_scores[action_name_without_trial] = combined_score

            # for k, v in action_scores.items():
            #     action_scores[k] = float('nan') if v['frame_count'] == 0 else (v['total_loss'] / v['frame_count'])

            return action_scores
        # import ipdb;ipdb.set_trace
        subject_scores = {
            'Average': evaluate_by_actions(self, per_pose_error)
        }

        # for subject_idx in range(len(self.labels['subject_names'])):
        #     subject_mask = self.labels['table']['subject_idx'] == subject_idx
        #     subject_scores[self.labels['subject_names'][subject_idx]] = \
        #         evaluate_by_actions(self, per_pose_error, subject_mask)

        return subject_scores

    def evaluate(self, keypoints_3d_predicted, Indexes, split_by_subject=False, transfer_cmu_to_human36m=False, transfer_human36m_to_human36m=False):
        """get differences between ground truth and predicted"""
        # import ipdb;ipdb.set_trace()
        keypoints_gt = self.Dataset.GetGTArray(Indexes)
        
        # keypoints_gt = self.labels['table']['keypoints'][:, :self.num_keypoints]
        if keypoints_3d_predicted.shape != keypoints_gt.shape:
            raise ValueError(
                '`keypoints_3d_predicted` shape should be %s, got %s' % \
                (keypoints_gt.shape, keypoints_3d_predicted.shape))

        # if transfer_cmu_to_human36m or transfer_human36m_to_human36m:
        #     human36m_joints = [10, 11, 15, 14, 1, 4]
        #     if transfer_human36m_to_human36m:
        #         cmu_joints = [10, 11, 15, 14, 1, 4]
        #     else:
        #         cmu_joints = [10, 8, 9, 7, 14, 13]

            # keypoints_gt = keypoints_gt[:, human36m_joints]
            # keypoints_3d_predicted = keypoints_3d_predicted[:, cmu_joints]

        # mean error per 16/17 joints in mm, for each pose
        per_pose_error = np.sqrt(((keypoints_gt - keypoints_3d_predicted) ** 2).sum(2)).mean(1)

        # relative mean error per 16/17 joints in mm, for each pose
        # if not (transfer_cmu_to_human36m or transfer_human36m_to_human36m):
        #     root_index = 6 if self.kind == "mpii" else 6
        # else:
        root_index = 2 #this is index of all points relative to this, for human 3.6M is pelvis.
        ##Now set to 2, which is top keel--> bot keel (in the middle)

        keypoints_gt_relative = keypoints_gt - keypoints_gt[:, root_index:root_index + 1, :]
        keypoints_3d_predicted_relative = keypoints_3d_predicted - keypoints_3d_predicted[:, root_index:root_index + 1, :]

        per_pose_error_relative = np.sqrt(((keypoints_gt_relative - keypoints_3d_predicted_relative) ** 2).sum(2)).mean(1)

        result = {
            'per_pose_error': self.evaluate_using_per_pose_error(per_pose_error, split_by_subject),
            'per_pose_error_relative': self.evaluate_using_per_pose_error(per_pose_error_relative, split_by_subject)
        }
        # import ipdb;ipdb.set_trace()

        return result['per_pose_error_relative']['Average']["Average"]["Average"], result




if __name__ == "__main__":
    dataset = MultiPigeon3D_Dataset(DatasetPath = "/home/alexchan/Documents/SampleDatasets/ImageTrainingData/N5000/",
                                    JSONPath="/home/alexchan/Documents/SampleDatasets/ImageTrainingData/N5000/Annotation/Train-3D-Individual.json",
                                    Type = "Train")
    